<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Personal Portfolio</title>
    <link rel="stylesheet" href="home.css">
</head>
<body>
    <header>
        <nav>
            <ul>
                <li><a href="http://127.0.0.1:5500/Profile_Enhancement/skills.html">Skills</a></li>
                <li><a href="http://127.0.0.1:5500/Profile_Enhancement/credentials.html">Credentials</a></li>
            </ul>
        </nav>
    </header>

    <main>
        <section id="home">
            <h1>Welcome to My Portfolio</h1>
            <img src="profile_pic.jpg" alt="Profile Picture" width="200">
            <p>Hi, I am Dare Fatade. Welcome to my personal portfolio website.</p>
        </section>

        <section id="about">
            <h2>About Me</h2>
            <p>I migrated from Nigeria to the United States in 2001, I will introduce myself in seven words. Passionate worker, responsible husband, and best colleague ever. That sums it up, though the last one needs confirmation from my former colleagues. I dream of working at a big tech company, working on projects that will change the life of billions of people, having an impact on the world with my work. I am fueled by my passion for understanding the nuances of technology. I consider myself a ‘forever student,’ eager to build on my foundations in technology and stay in tune with the latest technologies through continued coursework. I graduated with a Bachelor's Degree in Biology from Augusta University, and I got my Master’s Degree in Computer Science from City University of Seattle.
                 </p>
        </section>

        <section id="archive">
            <h2>Portfolio Archive</h2>
            <div>
                <h3> Kafka / Spark Streaming</h3>
                <p>Used Scala, Kafka, Spark Streaming and Spark SQL / Data Frames to process data streams of the Revature Capstone data wherein the data for Screeners, Recruiters, Qualified Lead, Contact Attempts, Screening and Offers are to be generated via a "producer" program that will push all the data to Kafka topics, and created a consumer program for these data streams and analyze the data and display results on the console.</p>

                <p>Environments: Apache Spark, Spark SQL, Kafka</p>
            </div>

            <div>
                <h3>Big Data - Finding Trends via Covid Data Analysis</h3>
                <p>Created a Spark Application that processes the provided Covid data.
                    The project involved some analysis of covid data (Every concept of spark from rdd, data frames, SQL, dataset and optimization methods was included, persistence also). The final expected output was different trends observed as part of data collectively and how and WHO can make use of these trends to make some useful decisions.
                    </p>

                    <p>Environments: Spark, Spark SQL, Data Analysis</p>
            </div>

            <div>
                <h3>Hive/MapReduce</h3>
                <p>Enabling Hive features with querying and optimizations where the main focal point is to understand the organization and positioning of data. Followed by querying the data, loading it into HDFS, and querying it into Hive. All these processes are accomplished on Ubuntu</p>

                <p>Environments: HDFS, Hive, Hadoop</p>
            </div>
            
        </section>
    </main>

    <footer>
        <p>&copy; Dare Fatade 2024. All rights reserved.</p>
    </footer>
</body>
</html>
